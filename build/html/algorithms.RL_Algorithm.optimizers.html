
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>algorithms.RL_Algorithm.optimizers package &#8212; AutoEnv 0.0.1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="algorithms.RL_Algorithm.optimizers.utils package" href="algorithms.RL_Algorithm.optimizers.utils.html" />
    <link rel="prev" title="algorithms.RL_Algorithm.GAIL package" href="algorithms.RL_Algorithm.GAIL.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="algorithms-rl-algorithm-optimizers-package">
<h1>algorithms.RL_Algorithm.optimizers package<a class="headerlink" href="#algorithms-rl-algorithm-optimizers-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html">algorithms.RL_Algorithm.optimizers.utils package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils.math">algorithms.RL_Algorithm.optimizers.utils.math module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils.replay_memory">algorithms.RL_Algorithm.optimizers.utils.replay_memory module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils.tools">algorithms.RL_Algorithm.optimizers.utils.tools module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils.torch">algorithms.RL_Algorithm.optimizers.utils.torch module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils.zfilter">algorithms.RL_Algorithm.optimizers.utils.zfilter module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.RL_Algorithm.optimizers.utils.html#module-algorithms.RL_Algorithm.optimizers.utils">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-algorithms.RL_Algorithm.optimizers.trpo">
<span id="algorithms-rl-algorithm-optimizers-trpo-module"></span><h2>algorithms.RL_Algorithm.optimizers.trpo module<a class="headerlink" href="#module-algorithms.RL_Algorithm.optimizers.trpo" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.conjugate_gradients">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">conjugate_gradients</code><span class="sig-paren">(</span><em class="sig-param">Avp_f</em>, <em class="sig-param">b</em>, <em class="sig-param">nsteps</em>, <em class="sig-param">rdotr_tol=1e-10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/algorithms/RL_Algorithm/optimizers/trpo.html#conjugate_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.conjugate_gradients" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Avp_f</strong> – Hessian Vector function</p></li>
<li><p><strong>b</strong> – negative loss gradient</p></li>
<li><p><strong>nsteps</strong> – how many steps to search</p></li>
<li><p><strong>rdotr_tol</strong> – the minimum improvement of rdotr</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.line_search">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">line_search</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">f</em>, <em class="sig-param">x</em>, <em class="sig-param">fullstep</em>, <em class="sig-param">expected_improve_full</em>, <em class="sig-param">max_backtracks=10</em>, <em class="sig-param">accept_ratio=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/algorithms/RL_Algorithm/optimizers/trpo.html#line_search"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.line_search" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – our policy model</p></li>
<li><p><strong>f</strong> – evaluation function</p></li>
<li><p><strong>x</strong> – params of the model</p></li>
<li><p><strong>fullstep</strong> – full step size</p></li>
<li><p><strong>expected_improve_full</strong> – expected improve</p></li>
<li><p><strong>max_backtracks</strong> – max iterative steps .5^n</p></li>
<li><p><strong>accept_ratio</strong> – accepted improving rate</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a boolean var indicating if the update step is success, if true return new param</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.ones">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">ones</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em>, <em class="sig-param">out=None</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">layout=torch.strided</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.ones" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>1</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>sizes (int…): a sequence of integers defining the shape of the output tensor.</dt><dd><p>Can be a variable number of arguments or a collection like a list or tuple.</p>
</dd>
</dl>
<p>out (Tensor, optional): the output tensor
dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</p>
<blockquote>
<div><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>).</p>
</div></blockquote>
<dl class="simple">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 1.,  1.,  1.],</span>
<span class="go">        [ 1.,  1.,  1.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.tensor">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">tensor</code><span class="sig-paren">(</span><em class="sig-param">data</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em>, <em class="sig-param">pin_memory=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs a tensor with <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code> always copies <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>. If you have a Tensor
<code class="docutils literal notranslate"><span class="pre">data</span></code> and want to avoid a copy, use <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.requires_grad_()</span></code>
or <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.Tensor.detach()</span></code>.
If you have a NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code> and want to avoid a copy, use
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.as_tensor()</span></code>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When data is a tensor <cite>x</cite>, <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.tensor()</span></code> reads out ‘the data’ from whatever it is passed,
and constructs a leaf variable. Therefore <code class="docutils literal notranslate"><span class="pre">torch.tensor(x)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach()</span></code>
and <code class="docutils literal notranslate"><span class="pre">torch.tensor(x,</span> <span class="pre">requires_grad=True)</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">x.clone().detach().requires_grad_(True)</span></code>.
The equivalents using <code class="docutils literal notranslate"><span class="pre">clone()</span></code> and <code class="docutils literal notranslate"><span class="pre">detach()</span></code> are recommended.</p>
</div>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>data (array_like): Initial data for the tensor. Can be a list, tuple,</dt><dd><p>NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, scalar, and other types.</p>
</dd>
<dt>dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</dt><dd><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, infers data type from <code class="xref py py-attr docutils literal notranslate"><span class="pre">data</span></code>.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>pin_memory (bool, optional): If set, returned tensor would be allocated in</dt><dd><p>the pinned memory. Works only for CPU tensors. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.2</span><span class="p">]])</span>
<span class="go">tensor([[ 0.1000,  1.2000],</span>
<span class="go">        [ 2.2000,  3.1000],</span>
<span class="go">        [ 4.9000,  5.2000]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># Type inference on data</span>
<span class="go">tensor([ 0,  1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.11111</span><span class="p">,</span> <span class="mf">0.222222</span><span class="p">,</span> <span class="mf">0.3333333</span><span class="p">]],</span>
<span class="go">                 dtype=torch.float64,</span>
<span class="go">                 device=torch.device(&#39;cuda:0&#39;))  # creates a torch.cuda.DoubleTensor</span>
<span class="go">tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device=&#39;cuda:0&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.14159</span><span class="p">)</span>  <span class="c1"># Create a scalar (zero-dimensional tensor)</span>
<span class="go">tensor(3.1416)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>  <span class="c1"># Create an empty tensor (of size (0,))</span>
<span class="go">tensor([])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.trpo_step">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">trpo_step</code><span class="sig-paren">(</span><em class="sig-param">policy_net</em>, <em class="sig-param">states</em>, <em class="sig-param">actions</em>, <em class="sig-param">advantages</em>, <em class="sig-param">max_kl</em>, <em class="sig-param">damping</em>, <em class="sig-param">use_fim=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/algorithms/RL_Algorithm/optimizers/trpo.html#trpo_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.trpo_step" title="Permalink to this definition">¶</a></dt>
<dd><p>optimize param of policy net given states and actions and advantages using TRPO</p>
</dd></dl>

<dl class="function">
<dt id="algorithms.RL_Algorithm.optimizers.trpo.zeros">
<code class="sig-prename descclassname">algorithms.RL_Algorithm.optimizers.trpo.</code><code class="sig-name descname">zeros</code><span class="sig-paren">(</span><em class="sig-param">*sizes</em>, <em class="sig-param">out=None</em>, <em class="sig-param">dtype=None</em>, <em class="sig-param">layout=torch.strided</em>, <em class="sig-param">device=None</em>, <em class="sig-param">requires_grad=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#algorithms.RL_Algorithm.optimizers.trpo.zeros" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a tensor filled with the scalar value <cite>0</cite>, with the shape defined
by the variable argument <code class="xref py py-attr docutils literal notranslate"><span class="pre">sizes</span></code>.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>sizes (int…): a sequence of integers defining the shape of the output tensor.</dt><dd><p>Can be a variable number of arguments or a collection like a list or tuple.</p>
</dd>
</dl>
<p>out (Tensor, optional): the output tensor
dtype (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional): the desired data type of returned tensor.</p>
<blockquote>
<div><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses a global default (see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>).</p>
</div></blockquote>
<dl class="simple">
<dt>layout (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.layout</span></code>, optional): the desired layout of returned Tensor.</dt><dd><p>Default: <code class="docutils literal notranslate"><span class="pre">torch.strided</span></code>.</p>
</dd>
<dt>device (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>, optional): the desired device of returned tensor.</dt><dd><p>Default: if <code class="docutils literal notranslate"><span class="pre">None</span></code>, uses the current device for the default tensor type
(see <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.set_default_tensor_type()</span></code>). <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</dd>
<dt>requires_grad (bool, optional): If autograd should record operations on the</dt><dd><p>returned tensor. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 0.,  0.,  0.],</span>
<span class="go">        [ 0.,  0.,  0.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([ 0.,  0.,  0.,  0.,  0.])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-algorithms.RL_Algorithm.optimizers">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-algorithms.RL_Algorithm.optimizers" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">AutoEnv</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="src.html">src package</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="algorithms.html">algorithms package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="algorithms.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="algorithms.AGen.html">algorithms.AGen package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="algorithms.RL_Algorithm.html">algorithms.RL_Algorithm package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="algorithms.RL_Algorithm.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="algorithms.RL_Algorithm.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="algorithms.RL_Algorithm.html#module-algorithms.RL_Algorithm.utils">algorithms.RL_Algorithm.utils module</a></li>
<li class="toctree-l4"><a class="reference internal" href="algorithms.RL_Algorithm.html#module-algorithms.RL_Algorithm">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="algorithms.dataset.html">algorithms.dataset package</a></li>
<li class="toctree-l3"><a class="reference internal" href="algorithms.distribution.html">algorithms.distribution package</a></li>
<li class="toctree-l3"><a class="reference internal" href="algorithms.policy.html">algorithms.policy package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#module-algorithms.utils">algorithms.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="algorithms.html#module-algorithms">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="envs.html">envs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_extractor.html">feature_extractor package</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">preprocessing package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  <li><a href="algorithms.html">algorithms package</a><ul>
  <li><a href="algorithms.RL_Algorithm.html">algorithms.RL_Algorithm package</a><ul>
      <li>Previous: <a href="algorithms.RL_Algorithm.GAIL.html" title="previous chapter">algorithms.RL_Algorithm.GAIL package</a></li>
      <li>Next: <a href="algorithms.RL_Algorithm.optimizers.utils.html" title="next chapter">algorithms.RL_Algorithm.optimizers.utils package</a></li>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, JackFram.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/algorithms.RL_Algorithm.optimizers.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>